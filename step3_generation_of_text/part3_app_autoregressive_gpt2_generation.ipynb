{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Generation step by step: No rock left unturned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of text is one of the most interesting visible parts of a model like GPT.  For the most part I have been using the OpenAI generation API, or the Huggingface APIs, but I had never dived deep into the 'behind the scenes' of the generation.\n",
    "\n",
    "In this notebook I do that: First I recreate the current Huggingface classes mainly used for generation, and then I do my own generation, step by step.\n",
    "\n",
    "**By the end of this notebook you will have a perfect understanding on how the Generation works, and will have created your own *Huggingface* classes to do text generation with you models!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's review first the Huggingface classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface makes it very easy to use the models. Their classes create a good level of abstraction which allow us to do anything with the call of a single function.\n",
    "\n",
    "For me, though, this is sometimes a cause of concern, because I can get what I want but I don't know how this is working behind the scenes.\n",
    "\n",
    "In this case, I want to review the classes the Huggingface provides to generate text from an initial prompt. I will review 2 classes, one that has a super high level of abstraction, to the point that I only need two sentences, and then the next one which requires a bit more of code from my part, but still has a 'magic' function called 'generate' that does all the generation behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level of abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Once upon a time, my father had been in that role. He was not a typical member of this kind of club. The manager was there and asked me: 'Can I join?\\n\\n'I said: 'It looks like you are\"}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "set_seed(42)\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator(\"Once upon a time,\", max_length=50, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium level of abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n",
      "\n",
      "The world that was created was not the same\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Input text\n",
    "input_text = \"Once upon a time,\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate text based on the input\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUT: What's going on behind the scenes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classes above are great, and save us a lot of time. But, what is going on behind the scenes? \n",
    "\n",
    "Lets uncover this magic, leaving no rock unturned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Instantiate the model and its tokenizer\n",
    "\n",
    "We will start by instantiating the Tokenizer and a model, the GPT2, as a starting point.\n",
    "\n",
    "The Tokenizer has its own magic and we can discuss the details of it in a different notebook.\n",
    "\n",
    "As for the model, what we are doing here is taking the GPT2 model from Huggingface. Fortunately in Part 1 we already learned what is behind this class and we know now what 'model' includes, and how it is done. In case you have any doubt about this, [please check Part 1](https://github.com/jcolano/transformer_step_by_step.git) where this is treated in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define the prompt \n",
    "We will define the following prompt for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Once upon a time,\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Convert the prompt into token ids\n",
    "Models can only take numbers, so the tokenizer takes the text, converts it into tokens, and then returns the integers that represent these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640,   11]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "input_ids "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Forward pass\n",
    "In [Part 1](https://github.com/jcolano/transformer_step_by_step.git) and [Part 2](https://github.com/jcolano/transformer_step_by_step.git) we learned how the model is built, what is the 'forward' function in the model, and how it is invoked. While doing generation we use the very same function to get the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the forward pass includes several components, typically: the logits, the hidden states, attention weights, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Get the logits\n",
    "For the case of the generation we only care about the logits, so we get them out of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits are the 'raw scores' or the 'raw probabilities' assigned to eack token in the entire vocabulary of the model for being the next position in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -34.5645,  -34.4081,  -38.3079,  ...,  -41.6996,  -39.7801,\n",
       "           -35.0521],\n",
       "         [ -84.7256,  -82.9326,  -87.0165,  ...,  -91.6667,  -86.2354,\n",
       "           -84.7094],\n",
       "         [-109.0798, -105.7258, -109.9115,  ..., -114.2847, -107.6933,\n",
       "          -105.3613],\n",
       "         [ -57.8935,  -58.5540,  -64.7374,  ...,  -64.9437,  -62.9294,\n",
       "           -60.0625],\n",
       "         [ -97.2383,  -98.3015, -100.6650,  ...,  -99.0754,  -98.9562,\n",
       "           -95.6314]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 50257])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Apply softmax to the logits\n",
    "We apply softmax to the raw probabilities to convert them into normalized probabilities (probabilities between 0 and 1 - all sum up to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_probs = F.softmax(logits[:, -1, :], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the normalized probabilities for all tokens. Each number has a value between 0 and 1, and the sum of all is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.0201e-06, 2.7697e-06, 2.6060e-07,  ..., 1.2774e-06, 1.4391e-06,\n",
       "         3.9997e-05]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_of_elements = torch.sum(token_probs)\n",
    "sum_of_elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of elements in the tensor\n",
    "num_elements = token_probs.numel()\n",
    "num_elements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Out of the token_probs pick the one with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = torch.multinomial(token_probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[339]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The value of this next_token is an integer that can be converted into a token.\n",
    "next_token "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the \"next_token\" tensor into a string. Every time we run the last few lines, the str_next_token will most probably be a different word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " he\n"
     ]
    }
   ],
   "source": [
    "str_next_token = tokenizer.decode(next_token[0])\n",
    "print(str_next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Concatenate the new token to the input_ids, and start this cycle again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the next token, we concatenate it to the previous string. For example:\n",
    "\n",
    "If previous string is: \"Once upon a time,\" \n",
    "... and new token is: \"there\"\n",
    "... then the new input string will be: \"Once upon a time, there\"\n",
    "\n",
    "And with this new input string we start again.\n",
    "\n",
    "Here we don't concatenate the next_token as a string but instead we concatenate it to the already converted to input_ids object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat([input_ids, next_token], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640,   11,  262,  339]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids \n",
    "\n",
    "#tensor([[7454, 2402,  257,  640,   11]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together in a loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done a manual, step-by-step, generation of one token, all that we have to do is put it inside a loop and repeat it while the input is less than 'max_length':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, they took fire, with a tiny bullet penetrating their form.\n",
      "\n",
      "Upon a very distant thread, almost a hundred and fifty thousand years ago, in the forbidding night of years when\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the input_ids with your starting text\n",
    "input_text = \"Once upon a time,\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Define the maximum length of the generated text\n",
    "max_length = 50\n",
    "\n",
    "# Generate text\n",
    "while len(input_ids[0]) < max_length:\n",
    "    # Get logits from the model\n",
    "    output = model(input_ids)\n",
    "\n",
    "    logits = output.logits\n",
    "\n",
    "    # Apply softmax to obtain token probabilities\n",
    "    token_probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "    # Sample the next token based on probabilities\n",
    "    next_token = torch.multinomial(token_probs, 1)\n",
    "\n",
    "    # Append the sampled token to input_ids\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND THAT'S IT!!!!\n",
    "\n",
    "That is how the \"GENERATE\" works behind the scenes. \n",
    "\n",
    "There are other parts to it that we will see now:\n",
    "\n",
    "- Temperature\n",
    "- TOP_K\n",
    "- TOP_P\n",
    "- REPETITION_PENALTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets apply temperature!\n",
    "To apply temperature when sampling from the model, you can scale the token probabilities before sampling. The temperature parameter controls the level of randomness in the sampling process. A higher temperature (e.g., 1.0) makes the sampling more random, while a lower temperature (e.g., 0.5) makes it more deterministic.\n",
    "\n",
    "*Higher Temperature (e.g., > 1.0)*: When you increase the temperature, it has a smoothing effect on the token probabilities. It makes the distribution of probabilities flatter, giving tokens with lower probabilities a better chance of being selected. This increases the randomness of the generated text. It's like injecting more randomness into the generation process.\n",
    "\n",
    "*Lower Temperature (e.g., < 1.0)*: Conversely, when you decrease the temperature, it sharpens the token probabilities. It makes the distribution peakier, with higher probabilities assigned to the most likely tokens. This results in more deterministic and focused text generation. Lower temperature values make the model more conservative and less prone to generating diverse, unexpected text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, there was a goddess, one that was truly known to mankind feathers. It was called the Mina Mina, and she was goddess of the sun, the moon, and earth.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the input_ids with your starting text\n",
    "input_text = \"Once upon a time, in a land far, far away,\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Define the maximum length of the generated text\n",
    "max_length = 50\n",
    "\n",
    "# Define the temperature\n",
    "temperature = 0.7  # Adjust the temperature value as desired\n",
    "\n",
    "# Generate text\n",
    "while len(input_ids[0]) < max_length:\n",
    "    # Get logits from the model\n",
    "    output = model(input_ids)\n",
    "\n",
    "    logits = output.logits\n",
    "\n",
    "    # Apply softmax with temperature to obtain token probabilities\n",
    "    token_probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "\n",
    "    # Sample the next token based on probabilities\n",
    "    next_token = torch.multinomial(token_probs, 1)\n",
    "\n",
    "    # Append the sampled token to input_ids\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now lets apply TOP_K\n",
    "\n",
    "Setting top_k to a positive integer value will ensure that only the top-k most likely tokens are considered for sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, with its ruins, its ruins, and its ruins, in the mountains of the great lakes of this country, the gods of the land, and of the world, and of the world\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the input_ids with your starting text\n",
    "input_text = \"Once upon a time, in a land far, far away,\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Define the maximum length of the generated text\n",
    "max_length = 50\n",
    "\n",
    "# Define the temperature\n",
    "temperature = 0.7  # Adjust the temperature value as desired\n",
    "\n",
    "# Define the top-k value\n",
    "top_k = 50  # Adjust the top-k value as desired\n",
    "\n",
    "# Generate text\n",
    "while len(input_ids[0]) < max_length:\n",
    "    # Get logits from the model\n",
    "    output = model(input_ids)\n",
    "\n",
    "    logits = output.logits\n",
    "\n",
    "    # Apply softmax with temperature to obtain token probabilities\n",
    "    token_probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "\n",
    "    # Apply top-k filtering to the token probabilities\n",
    "    # After softmax, you have token probabilities for each token in the model's vocabulary.\n",
    "    # top_k specifies how many of the highest probability tokens you want to consider. It selects the top k tokens with the highest probabilities.\n",
    "    # filtered_token_probs contains the probabilities of the top k tokens, and top_indices contains the corresponding indices of these top tokens.\n",
    "    filtered_token_probs, top_indices = token_probs.topk(top_k, dim=-1)\n",
    "\n",
    "    # Sample the next token based on the filtered probabilities\n",
    "    # torch.multinomial is used to randomly sample one token based on the probabilities in filtered_token_probs. It's essentially drawing a token from the filtered distribution.\n",
    "    # The 1 passed as the second argument specifies that you want to draw one token.\n",
    "    next_token = torch.multinomial(filtered_token_probs, 1)\n",
    "\n",
    "    # Map the sampled token back to the original token space\n",
    "    # Once you've sampled an index from the filtered distribution, you use top_indices to map it back to the index in the original token vocabulary.\n",
    "    # This step ensures that the generated token corresponds to a valid token in the model's vocabulary.\n",
    "    next_token = top_indices.gather(dim=-1, index=next_token)\n",
    "\n",
    "    # Append the sampled token to input_ids\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets apply TOP_P\n",
    "You can apply the top_p parameter (also known as nucleus sampling) to control the cumulative probability mass of the tokens considered during sampling. Setting top_p to a value between 0 and 1 ensures that only the most probable tokens whose cumulative probability mass exceeds top_p are considered for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, the world was a land forever changed.\n",
      "Then came we.\n",
      "The world was a land forever forgotten. After all here was a land forever changed.\n",
      "And now, here is\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the input_ids with your starting text\n",
    "input_text = \"Once upon a time, in a land far, far away,\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Define the maximum length of the generated text\n",
    "max_length = 50\n",
    "\n",
    "# Define the temperature\n",
    "temperature = 0.7  # Adjust the temperature value as desired\n",
    "\n",
    "# Define the top-k value\n",
    "top_k = 50  # Adjust the top-k value as desired\n",
    "\n",
    "# Define the top-p value\n",
    "top_p = 0.9  # Adjust the top-p value as desired\n",
    "\n",
    "# Generate text\n",
    "while len(input_ids[0]) < max_length:\n",
    "    # Get logits from the model\n",
    "    output = model(input_ids)\n",
    "\n",
    "    logits = output.logits\n",
    "\n",
    "    # Apply softmax with temperature to obtain token probabilities\n",
    "    token_probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "\n",
    "    # Apply top-k filtering to the token probabilities\n",
    "    filtered_token_probs, top_indices = token_probs.topk(top_k, dim=-1)\n",
    "\n",
    "    # Apply top-p filtering to the filtered token probabilities\n",
    "    sorted_filtered_probs, sorted_filtered_indices = torch.sort(filtered_token_probs, descending=True, dim=-1)\n",
    "    cumulative_filtered_probs = torch.cumsum(sorted_filtered_probs, dim=-1)\n",
    "    exceed_top_p = cumulative_filtered_probs > top_p\n",
    "    min_filtered_exceed_index = torch.min(sorted_filtered_indices.masked_fill(exceed_top_p, sorted_filtered_indices.size(-1) - 1), dim=-1).values\n",
    "    final_token_probs = torch.zeros_like(filtered_token_probs)\n",
    "    final_token_probs.scatter_(dim=-1, index=min_filtered_exceed_index.unsqueeze(-1), src=filtered_token_probs.gather(dim=-1, index=min_filtered_exceed_index.unsqueeze(-1)))\n",
    "\n",
    "    # Sample the next token based on the final filtered probabilities\n",
    "    next_token = torch.multinomial(final_token_probs, 1)\n",
    "\n",
    "    # Map the sampled token back to the original token space\n",
    "    next_token = top_indices.gather(dim=-1, index=next_token)\n",
    "\n",
    "    # Append the sampled token to input_ids\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding repetition_penalty\n",
    "\n",
    "Lets add a repetition penalty to the code to discourage the model from generating repeated tokens. \n",
    "\n",
    "When applying a repetition penalty, it's a common practice to consider not just the last token but a window of the last n tokens to prevent repetitive patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, the world was a land forever changed.\n",
      "Then came we.\n",
      "The world was a land forever forgotten. After all here was a land forever changed.\n",
      "And now, here is\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the input_ids with your starting text\n",
    "input_text = \"Once upon a time, in a land far, far away,\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Define the maximum length of the generated text\n",
    "max_length = 50\n",
    "\n",
    "# Define the temperature\n",
    "temperature = 0.7  # Adjust the temperature value as desired\n",
    "\n",
    "# Define the top-k value\n",
    "top_k = 50  # Adjust the top-k value as desired\n",
    "\n",
    "# Define the top-p value\n",
    "top_p = 0.9  # Adjust the top-p value as desired\n",
    "\n",
    "# Define the repetition penalty\n",
    "repetition_penalty = 1.2  # Adjust the penalty value as desired\n",
    "\n",
    "# Generate text\n",
    "while len(input_ids[0]) < max_length:\n",
    "    # Get logits from the model\n",
    "    output = model(input_ids)\n",
    "\n",
    "    logits = output.logits\n",
    "\n",
    "    # Apply softmax with temperature to obtain token probabilities\n",
    "    token_probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "\n",
    "    # Apply top-k filtering to the token probabilities\n",
    "    filtered_token_probs, top_indices = token_probs.topk(top_k, dim=-1)\n",
    "\n",
    "    # Apply top-p filtering to the filtered token probabilities\n",
    "    sorted_filtered_probs, sorted_filtered_indices = torch.sort(filtered_token_probs, descending=True, dim=-1)\n",
    "    cumulative_filtered_probs = torch.cumsum(sorted_filtered_probs, dim=-1)\n",
    "    exceed_top_p = cumulative_filtered_probs > top_p\n",
    "    min_filtered_exceed_index = torch.min(sorted_filtered_indices.masked_fill(exceed_top_p, sorted_filtered_indices.size(-1) - 1), dim=-1).values\n",
    "    final_token_probs = torch.zeros_like(filtered_token_probs)\n",
    "    final_token_probs.scatter_(dim=-1, index=min_filtered_exceed_index.unsqueeze(-1), src=filtered_token_probs.gather(dim=-1, index=min_filtered_exceed_index.unsqueeze(-1)))\n",
    "\n",
    "    # Calculate the repetition penalty and apply it to the token probabilities\n",
    "    last_token = input_ids[:, -1]\n",
    "    repetition_penalty_tensor = torch.where(last_token.unsqueeze(-1) == top_indices, repetition_penalty, torch.tensor(1.0, device=input_ids.device))\n",
    "    token_probs = final_token_probs * repetition_penalty_tensor\n",
    "\n",
    "    # Sample the next token based on the token probabilities\n",
    "    next_token = torch.multinomial(token_probs, 1)\n",
    "\n",
    "    # Map the sampled token back to the original token space\n",
    "    next_token = top_indices.gather(dim=-1, index=next_token)\n",
    "\n",
    "    # Append the sampled token to input_ids\n",
    "    input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together in a more organized way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have now most of the bells and whistles of the GENERATOR, we can organize it as a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far, far away, the world was a land forever changed.\n",
      "Then came we.\n",
      "The world was a land forever forgotten. After all here was a land forever changed.\n",
      "And now, here is\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, max_length=50, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.2, repetition_window=5):\n",
    "        self.max_length = max_length\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.repetition_penalty = repetition_penalty\n",
    "        self.repetition_window = repetition_window\n",
    "\n",
    "\n",
    "def generate(model, input_ids, config):\n",
    "    while len(input_ids[0]) < max_length:\n",
    "        # Get logits from the model\n",
    "        output = model(input_ids)\n",
    "\n",
    "        logits = output.logits\n",
    "\n",
    "        # Apply softmax with temperature to obtain token probabilities\n",
    "        token_probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "\n",
    "        # Apply top-k filtering to the token probabilities\n",
    "        filtered_token_probs, top_indices = token_probs.topk(top_k, dim=-1)\n",
    "\n",
    "        # Apply top-p filtering to the filtered token probabilities\n",
    "        sorted_filtered_probs, sorted_filtered_indices = torch.sort(filtered_token_probs, descending=True, dim=-1)\n",
    "        cumulative_filtered_probs = torch.cumsum(sorted_filtered_probs, dim=-1)\n",
    "        exceed_top_p = cumulative_filtered_probs > top_p\n",
    "        min_filtered_exceed_index = torch.min(sorted_filtered_indices.masked_fill(exceed_top_p, sorted_filtered_indices.size(-1) - 1), dim=-1).values\n",
    "        final_token_probs = torch.zeros_like(filtered_token_probs)\n",
    "        final_token_probs.scatter_(dim=-1, index=min_filtered_exceed_index.unsqueeze(-1), src=filtered_token_probs.gather(dim=-1, index=min_filtered_exceed_index.unsqueeze(-1)))\n",
    "\n",
    "        # Calculate the repetition penalty and apply it to the token probabilities\n",
    "        last_token = input_ids[:, -1]\n",
    "        repetition_penalty_tensor = torch.where(last_token.unsqueeze(-1) == top_indices, repetition_penalty, torch.tensor(1.0, device=input_ids.device))\n",
    "        token_probs = final_token_probs * repetition_penalty_tensor\n",
    "\n",
    "        # Sample the next token based on the token probabilities\n",
    "        next_token = torch.multinomial(token_probs, 1)\n",
    "\n",
    "        # Map the sampled token back to the original token space\n",
    "        next_token = top_indices.gather(dim=-1, index=next_token)\n",
    "\n",
    "        # Append the sampled token to input_ids\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text \n",
    "\n",
    "config = Config(\n",
    "    max_length=50, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.9, \n",
    "    repetition_penalty=1.2, \n",
    "    repetition_window=5\n",
    "    )\n",
    "\n",
    "# Initialize the input_ids with your starting text\n",
    "input_text = \"Once upon a time, in a land far, far away,\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "generated_text = generate(model, input_ids, config)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class to emulate Huggingface\n",
    "\n",
    "Lets take the code from above and replicate the \"Huggingface Pipeline\" class, with generator, config and all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyPipeline:\n",
    "    \"\"\"A class for generating text using a language model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The language model to use for text generation.\n",
    "        tokenizer: The tokenizer associated with the language model.\n",
    "        config (MyPipeline.Config, optional): Configuration for text generation. Defaults to None.\n",
    "\n",
    "        Configuration parameters for text generation.\n",
    "\n",
    "        MyPipeline.Config:\n",
    "\n",
    "        Args:\n",
    "            max_length (int, optional): The maximum length of generated text. Defaults to 50.\n",
    "            temperature (float, optional): The temperature for controlling randomness. Defaults to 0.7.\n",
    "            top_k (int, optional): The top-k value for filtering tokens. Defaults to 50.\n",
    "            top_p (float, optional): The top-p value for filtering tokens. Defaults to 0.9.\n",
    "            repetition_penalty (float, optional): The repetition penalty value. Defaults to 1.2.\n",
    "            repetition_window (int, optional): The repetition window size. Defaults to 5.\n",
    "\n",
    "    \"\"\"\n",
    "    class Config:\n",
    "\n",
    "        def __init__(self, max_length=50, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.2, repetition_window=5):\n",
    "            self.max_length = max_length\n",
    "            self.temperature = temperature\n",
    "            self.top_k = top_k\n",
    "            self.top_p = top_p\n",
    "            self.repetition_penalty = repetition_penalty\n",
    "            self.repetition_window = repetition_window\n",
    "\n",
    "    def __init__(self, model, tokenizer, config=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config if config is not None else MyPipeline.Config()\n",
    "\n",
    "    def __call__(self, input_text):  # Define the __call__ method\n",
    "        return self.generate(input_text)\n",
    "\n",
    "    def generate(self, input_text):\n",
    "        max_length = self.config.max_length\n",
    "        temperature = self.config.temperature\n",
    "        top_k = self.config.top_k\n",
    "        top_p = self.config.top_p\n",
    "        repetition_penalty = self.config.repetition_penalty\n",
    "\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors='pt')\n",
    "        \n",
    "        while len(input_ids[0]) < max_length:\n",
    "            # Get logits from the model\n",
    "            output = self.model(input_ids)\n",
    "\n",
    "            logits = output.logits\n",
    "\n",
    "            # Apply softmax with temperature to obtain token probabilities\n",
    "            token_probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "\n",
    "            # Apply top-k filtering to the token probabilities\n",
    "            filtered_token_probs, top_indices = token_probs.topk(top_k, dim=-1)\n",
    "\n",
    "            # Apply top-p filtering to the filtered token probabilities\n",
    "            sorted_filtered_probs, sorted_filtered_indices = torch.sort(filtered_token_probs, descending=True, dim=-1)\n",
    "            cumulative_filtered_probs = torch.cumsum(sorted_filtered_probs, dim=-1)\n",
    "            exceed_top_p = cumulative_filtered_probs > top_p\n",
    "            min_filtered_exceed_index = torch.min(sorted_filtered_indices.masked_fill(exceed_top_p, sorted_filtered_indices.size(-1) - 1), dim=-1).values\n",
    "            final_token_probs = torch.zeros_like(filtered_token_probs)\n",
    "            final_token_probs.scatter_(dim=-1, index=min_filtered_exceed_index.unsqueeze(-1), src=filtered_token_probs.gather(dim=-1, index=min_filtered_exceed_index.unsqueeze(-1)))\n",
    "\n",
    "            # Calculate the repetition penalty and apply it to the token probabilities\n",
    "            last_token = input_ids[:, -1]\n",
    "            repetition_penalty_tensor = torch.where(last_token.unsqueeze(-1) == top_indices, repetition_penalty, torch.tensor(1.0, device=input_ids.device))\n",
    "            token_probs = final_token_probs * repetition_penalty_tensor\n",
    "\n",
    "            # Sample the next token based on the token probabilities\n",
    "            next_token = torch.multinomial(token_probs, 1)\n",
    "\n",
    "            # Map the sampled token back to the original token space\n",
    "            next_token = top_indices.gather(dim=-1, index=next_token)\n",
    "\n",
    "            # Append the sampled token to input_ids\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        return generated_text \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We go full circle to high level of abstraction with our own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, in a land far, far away, the world was a land forever changed.\\nThen came we.\\nThe world was a land forever forgotten. After all here was a land forever changed.\\nAnd now, here is'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_generator = MyPipeline(model, tokenizer)\n",
    "my_generator(\"Once upon a time, in a land far, far away,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
